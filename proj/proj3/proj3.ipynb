{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"proj3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Data Transformation\n",
    "\n",
    "## Due Date: Wednesday, April 3rd, 5:00 PM\n",
    "\n",
    "## Assignment Details\n",
    "\n",
    "In this project, we'll be working with one month of data from sensors in buildings at UC Berkeley. This is a very typical real-world dataset—i.e. it's kind of a mess. The full dataset contains a giant `data` table of many billions of sensor readings over the course of a decade; we will look at a single month of that data. It also contains a variety of other tables that contextualize the readings.\n",
    "\n",
    "The **schema for the database** is shown below.\n",
    "* Each line represents a relationship between the two fields.\n",
    "* The side of the line diverging to three lines / arrows represents the **\"many\"** side of the relationship, while the side of the line converging to one arrow represents the **\"one\"** side of the relationship.\n",
    "* This file is available as `data/schema.png`.\n",
    "\n",
    "Occasionally people believe that when the data has a well-structured schema, then it's simple to proceed! *We'll put this assumption to the test.*\n",
    "\n",
    "Citation: Luo, N., Wang, Z., Blum, D. et al. A three-year dataset supporting research on building energy management and occupancy analytics. _Sci Data_ 9, 156 (2022). https://doi.org/10.1038/s41597-022-01257-x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/schema.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you make any queries to explore the data, do not forget to add a LIMIT clause; LIMIT 10 is a good default to always add to the end of your queries. Otherwise, your connection may close as a result of trying to load excessively many rows. To explore the data within the JupyterNotebook, you may add a cell like so:\n",
    "```\n",
    "%%sql\n",
    "SELECT ... \n",
    "LIMIT 10;\n",
    "```\n",
    "In addition, for the entirety of the project, you may make as many CTEs as you'd like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics & Scoring Breakdown\n",
    "\n",
    "This is an **individual project**. However, you’re welcome to collaborate with any other student in the class as long as it’s within the academic honesty guidelines. Free-response questions (marked 'm' in the table below) are manually graded.\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "1a\t| m: 1\n",
    "1b  | 1\n",
    "1c\t| 1\n",
    "1d\t| m: 2\n",
    "2a\t| 3\n",
    "2b\t| 1\n",
    "3a\t| m: 1\n",
    "3b\t| 1\n",
    "3c\t| 1\n",
    "3d  | m: 1\n",
    "3e  | 2\n",
    "4a\t| 2\n",
    "4b\t| 2\n",
    "4c\t| 3\n",
    "5a  | 1\n",
    "5b  | 1\n",
    "5c  | 3\n",
    "**Total** | 27\n",
    "\n",
    "**Grand Total:** 27 points (autograded: 22, manual: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%sql postgresql://jovyan@127.0.0.1:5432/postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Up the Database\n",
    "To load the database, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up the database\n",
    "\n",
    "# first clean existing connections\n",
    "import subprocess\n",
    "call = subprocess.run([\"psql\", \"-h\", \"localhost\", \\\n",
    "                       \"-tAc\", \"SELECT 1 FROM pg_database WHERE datname='ucb_buildings'\", \"template1\"], \\\n",
    "                      stdout=subprocess.PIPE, text=True)\n",
    "if call.stdout == \"1\\n\":\n",
    "    !psql postgresql://localhost:5432/ucb_buildings -c 'SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE datname = current_database()  AND pid <> pg_backend_pid();'\n",
    "\n",
    "# then delete and recreate database\n",
    "!psql -h localhost -c 'DROP DATABASE IF EXISTS ucb_buildings'\n",
    "!psql -h localhost -c 'CREATE DATABASE ucb_buildings'\n",
    "!gunzip -c data/proj3.sql.gz | psql -h localhost -d ucb_buildings -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cell to connect to the `ucb_buildings` database. There should be no errors after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://jovyan@127.0.0.1:5432/ucb_buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run the following cell, no further action is needed.\n",
    "from data101_utils import GradingUtil\n",
    "grading_util = GradingUtil(\"proj3\")\n",
    "grading_util.prepare_autograder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 1: Unboxing the Data\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "As mentioned above, we are working with just one month of data. In the full database (which we don't have access to), tables like the `data` table have billions of rows. What do you notice about the design of the database schema above that helps support the large amount of data and minimize redundancy? Keep your response to at most three sentences.\n",
    "\n",
    "**Hint:** There is no need to examine any data here. What is a technique learned in lecture 16? Define that technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 1b\n",
    "\n",
    "The diagram claims that `buildings_site_mapping` has a **many-to-many relationship** with `real_estate_metadata`. Let's validate that. \n",
    "\n",
    "Below is an example of the aggregate function `json_agg` being used with a table; you will need to do this in the next two parts. Note the necessary use of a `GROUP BY` to use `json_agg`, which aggregates values from the table and returns a single JSON array. [(Documentation)](https://www.postgresql.org/docs/9.5/functions-aggregate.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT b.site, json_agg(b) from buildings_site_mapping b GROUP BY b.site LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the **distinct** values of `buildings_site_mapping.building` that match multiple tuples in `real_estate_metadata.building_name`, and for each such value of `buildings_site_mapping.building` return the matches as JSON via `json_agg(real_estate_metadata)`. Your output should contain the building and the `json_agg` in that order. **Order your final result by building ascending**.\n",
    "\n",
    "**Hint:** You should use a CTE to find the distinct buildings of `buildings_site_mapping` before applying necessary table joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_1b result_1b <<\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_1b = %sqlcmd snippets query_1b\n",
    "grading_util.save_results(\"result_1b\", query_1b, result_1b)\n",
    "result_1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 1c\n",
    "\n",
    "Now find examples of many matches in the opposite direction. For each distinct `real_estate_metadata.building_name` value, find the ones that have multiple matches in `buildings_site_mapping.building`, and for each return a `json_agg` of the multiple values for `buildings_site_mapping`. Your output should contain the building name and the `json_agg` in that order. **Order your final result by building name ascending.**\n",
    "\n",
    "**Hint:** You should use a CTE to find the distinct building names of `real_estate_metadata` before applying necessary table joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_1c result_1c <<\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_1c = %sqlcmd snippets query_1c\n",
    "grading_util.save_results(\"result_1c\", query_1c, result_1c)\n",
    "result_1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 1d\n",
    "\n",
    "Do you see any issues with the schema given? In particular, please address the two questions below:\n",
    "- Can you uniquely determine the building given the sensor data? Why? (**Hint:** given a row in the `data` table, can you determine a **uniquely** associated row in `real_estate_metadata` table? Your answer should draw insights from 1b.)\n",
    "- Could `buildings_site_mapping.building` be a valid foreign key pointing to `real_estate_metadata.building_name`? (**Hint:** think about the definition / constraints of a foreign key.)\n",
    "\n",
    "Please keep your response to **at most three sentences.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Looking for Outliers in the Readings\n",
    "Physical sensors, such as the ones responsible for generating this data, are notorious for occasionally producing crazy outliers. In this section, we will undergo some data cleaning to address these outliers.\n",
    "\n",
    "The readings from all different types of sensors are mixed together within this `data` table. This assortment of mixed readings will require some extra work to identify the outliers. Let's get started.\n",
    "\n",
    "### Question 2a: Outlier Detection\n",
    "\n",
    "Let's start with finding the outlying values *for each sensor id*. We'll define an outlier as an observation that is more than **3 Hampel X84 intervals** away from the median. See [lecture 14](https://docs.google.com/presentation/d/1vG53ZkZfwWm2Bz2QFCHcyB5ySuUod5ESYgRKz0qXqjk/edit?usp=sharing) for more.\n",
    "\n",
    "Create a view `labeled_data` that contains all of the columns in `data` and adds three additional columns on the far right ***for each sensor id***:\n",
    "  - `median` containing the median using `percentile_disc`\n",
    "  - `mad` containing the Median Absolute Deviation (MAD),\n",
    "  - `outlier` that contains `true` for the outlier readings and `false` for the rest. **Also,** for data points where the `mad` is 0, set this to `false`.\n",
    "\n",
    "**Hint:** You should use CTEs to first compute the median and MAD values. Feel free to create as many CTEs as you need to complete the objective. Our staff solution used 3: one to compute the median of *each* sensor id, one to compute the absolute differences for each data point relative to its sensor id median, and another to finally compute the median of these absolute differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_2a result_2a <<\n",
    "CREATE OR REPLACE VIEW labeled_data AS\n",
    "...\n",
    ";\n",
    "SELECT * FROM labeled_data WHERE outlier ORDER BY id, time LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_2a = %sqlcmd snippets query_2a\n",
    "grading_util.save_results(\"result_2a\", query_2a, result_2a)\n",
    "result_2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b: Outlier Handling (Winsorization)\n",
    "\n",
    "In this step, define a view `cleaned_data` containing all the columns of `labeled_data` and one additional column on the far right called `clean_value`. This column will contain a copy of `labeled_data.value` if that value is not an outlier. For outliers, it should contain the value Winsorized to the nearest outlier boundary value (3 Hampel X84 intervals from the median). If the MAD is 0, then the cleaned value should be unchanged (i.e., the same as the original value).\n",
    "\n",
    "To reiterate, we consider a value to be an outlier if it is more than (strictly greater than or strictly less than) 3 Hampel X84 intervals from the median. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_2b result_2b <<\n",
    "CREATE OR REPLACE VIEW cleaned_data AS\n",
    "...\n",
    ";\n",
    "SELECT * FROM cleaned_data WHERE outlier ORDER BY id, time LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_2b = %sqlcmd snippets query_2b\n",
    "grading_util.save_results(\"result_2b\", query_2b, result_2b)\n",
    "result_2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Entity Resolution\n",
    "\n",
    "### Question 3a\n",
    "There is a lot of mess in this dataset related to entity names. As a start, have a look at all of the distinct values in the `units` field of the `metadata` table. What do you notice about these values? Are there any duplicates? **Limit your response to one sentence.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "Sometimes entity resolution is as simple as a text transformation. For example, how many unique `units` values are there, and how many would there be if we ignored case (upper vs. lower case)? Your output should be a table with one row and two columns; the first column should contain the number of unique `units` values, and the second column should contain the number of unique `units` values if we ignored case. The two columns can have arbitrary names—we will not be checking column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_3b result_3b <<\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_3b = %sqlcmd snippets query_3b\n",
    "grading_util.save_results(\"result_3b\", query_3b, result_3b)\n",
    "result_3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3c\n",
    "\n",
    "Arguably, we shouldn't care about these alternative unit labels, *as long as each sensor **class** uses a single value of `units` for all its sensor ids*. After all, maybe the capitalization means something to somebody!\n",
    "\n",
    "Write a SQL query that returns a **single row with one column** of value `true` if the condition (in italics above) holds, or a single row with one column of value `false` otherwise. This column can have an arbitrary name—we will not be checking its name. Please do not hard code this query—we reserve the right to penalize your score if you do so.\n",
    "\n",
    "**Hint:** You may find `ALL` or some other logical check such as `SUM(bool)  == len` to be useful here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_3c result_3c <<\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_3c = %sqlcmd snippets query_3c\n",
    "grading_util.save_results(\"result_3c\", query_3c, result_3c)\n",
    "result_3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3d\n",
    "\n",
    "Moving on, have a look at the `real_estate_metadata` table—starting with the distinct values in the `location` field! What do you notice about these values? Keep your response to at most two sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 3e\n",
    "\n",
    "It turns out this `real_estate_metadata` table is the result of an [OCR scan](https://en.wikipedia.org/wiki/Optical_character_recognition). We will just focus on cleaning up the `location` column for the time being, and leave you to imagine the effort required to do a full cleanup of all columns.\n",
    "\n",
    "We have preloaded Postgres' extension packages for \"fuzzy\" string matching (`fuzzystrmatch`) and trigrams (`pg_trgm`) for you so that you can take advantage of some convenient utility functions. You can use any of the string functions in those packages if you'd like ([as documented here for fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html) or [here for pg_trgm](https://www.postgresql.org/docs/current/pgtrgm.html)).\n",
    "\n",
    "We also created a lookup table of standardized names, `uc_locations`.\n",
    "\n",
    "Now, using any of the string functions you like (or none at all!), write a SQL query that returns the columns `(building_name, address, location, clean_location)` where `clean_location` contains the best match from `uc_locations.loc_name`. While we recommend taking the time to explore different string functions such as `levenshtein`, `word_similarity`, and `metaphone`, we have found `word_similarity` to be the most powerful in this scenario. You may find that you can't clean up everything with the string functions, so your view may have to include some specific logic for cases in the data that have to be handled \"manually\". You can choose to do this question in whatever manner you wish as long as your query does not use `CREATE TABLE`, `INSERT INTO`, or `UPDATE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_3e result_3e <<\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_3e = %sqlcmd snippets query_3e\n",
    "grading_util.save_results(\"result_3e\", query_3e, result_3e)\n",
    "result_3e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4: Interpolating Missing Data\n",
    "Real-world data, real-world problems. Our sensors should be reporting every 15 minutes, but you can be sure that we're missing some data. Here we will fix it. It's a bit more involved than what we looked at in class!\n",
    "\n",
    "### Question 4a: Finding missing readings\n",
    "In the `data` table, the `id` column identifies a unique sensor. Sensor readings should be recorded every 15 minutes from every sensor. Are we missing any readings, and if so which ones? We will focus on readings that are separated by at least 30 minutes or more; readings that are \\[0-30) minutes apart are considered to be fine.\n",
    "\n",
    "To answer this question you'll need to read up a bit on [SQL timestamps](https://www.postgresql.org/docs/current/datatype-datetime.html) and [Functions for manipulating datetime types](https://www.postgresql.org/docs/current/functions-datetime.html). Taking the time now to look at the documentation will save you time later on. Have a particular look at the following:\n",
    "- The [date_trunc](https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC) function will quantize times to the nearest unit of your choosing. For example, to round the `time` field to the nearest minute you can say `date_trunc('minute', time)`. **You'll need to quantize to minutes right away before you worry about missing readings.**\n",
    "- There are various ways to enter constant intervals of time as strings. For example, a 30 minute interval can be written as `interval '30 minutes'` or `'30 minutes'::interval`. See [date/time input](https://www.postgresql.org/docs/current/datatype-datetime.html#DATATYPE-INTERVAL-INPUT) for more info.\n",
    "- You can do arithmetic on date/time types [as documented here](https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC). That will handle all the weird periodicities of clocks and calendars for you. Pay attention to the input and output types of these functions!\n",
    "- You will need to use the [lag](https://www.geeksforgeeks.org/postgresql-lag-function/) function as the window function.\n",
    "\n",
    "\n",
    "Create a view called `gaps` that augments the `data` schema with three columns:\n",
    "- `lagtime` is the quantized time of the previous reading for that sensor (relative to the current row for a particular row)\n",
    "- `lagvalue` is the value of the previous reading for that sensor\n",
    "- `timediff` is the difference in quantized time between this reading and the previous reading\n",
    "\n",
    "The view should only contain rows where `timediff` is **greater than or equal to 30 minutes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_4a result_4a <<\n",
    "CREATE OR REPLACE VIEW gaps AS\n",
    "...\n",
    ";\n",
    "SELECT * FROM gaps ORDER BY id, time LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_4a = %sqlcmd snippets query_4a\n",
    "grading_util.save_results(\"result_4a\", query_4a, result_4a)\n",
    "result_4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b: Creating tuples for the missing readings\n",
    "Now we need to manufacture new tuples to fill in the gaps. For example, if you had a tuple from id `abc` timestamped at 1PM today and the next tuple in time from `abc` was timestamped at 1:45PM, you'll need to manufacture two new tuples with id `abc` and `NULL` values: one timestamped at 1:15PM and another timestamped at 1:30PM. We will worry about replacing the `NULL` values in the next step.\n",
    "\n",
    "To manufacture tuples not related to stored data in the database, we'll need to use a *table-valued function*. The table-valued function we want here is `generate_series` [(documented here)](https://www.postgresql.org/docs/current/functions-srf.html), which we will use to generate *and sequentially timestamp* the right number of tuples to match the number of tuples we found missing.\n",
    "\n",
    "To get a feel for `generate_series`, consider the following simple query that generates a table of integers with intervals of size 3 between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "  FROM generate_series(1, 10, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use `generate_series` in a `LATERAL JOIN` query: for each tuple on the left of the `LATERAL` it will produce a series based on the values of that tuple. So for example, we can generate 2 tuples for each tuple of `uc_locations` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT loc_id, loc_name, length(loc_name), newval\n",
    "  FROM uc_locations u, \n",
    "       LATERAL generate_series(length(loc_name), length(loc_name) + 2, 2) AS newval;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the 2 values it generates are the length of the `loc_name`, and the length + 2. You might want to play with the query above to make sure you understand the documentation for `generate_series` and `LATERAL`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, on to your task!\n",
    "\n",
    "Create a view `complete` that contains the tuples from `data` as well as new tuples that fill in any gaps greater than or equal to 30 minutes. Each gap should be filled by adding tuples in increments of 15 minutes from the *start* of the gap, **with `NULL` as the value**. You probably want to use your `gaps` view as well as `generate_series` to do this!\n",
    "\n",
    "**Hints:** \n",
    "- The lower and upper bounds in generate_series (in pseudocode) should be `(lagtime + 15 minutes, time - 15 minutes)`.\n",
    "- You may want to use UNION ALL to combine your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_4b result_4b <<\n",
    "CREATE OR REPLACE VIEW complete AS\n",
    "...\n",
    ";\n",
    "\n",
    "SELECT * FROM complete ORDER BY id, time LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_4b = %sqlcmd snippets query_4b\n",
    "grading_util.save_results(\"result_4b\", query_4b, result_4b)\n",
    "result_4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4c: Linear Interpolation\n",
    "*Note: If you struggled with the previous subparts of this problem, you can use our table `complete_provided` instead of your `complete` table in this subpart.*\n",
    "\n",
    "Now, given the `complete` view or the `complete_provided` table, the remaining task is to do linear interpolation to fill in the missing values we manufactured in Step 2. We have code from Lecture 16 (Data Cleaning II) we can adapt from and use here! In particular, your database already includes the UDA (User-Defined Aggregate) `coalesce_agg` we used in lecture (you can use it directly, there's no need to redefine it).\n",
    "\n",
    "But note that in Lecture 15's example of linear interpolation we had a field that was used to order *all* the records in the table. In contrast, here the ordering we care about is the series of `time` for each sensor `id` *independently*. **You will need to make minor changes to adapt the linear interpolation code from class to work here.**\n",
    "\n",
    "Create a view `likely_data` that contains all the tuples from `complete`, with an additional column called `interpolated` that contains a copy of `value` if it is non-NULL, otherwise an interpolated value based on linear interpolation **per sensor id over time**. Please **retain all additional columns created from forward and backward passes**. (There should be 10 columns in the view—the order of the columns does not matter.) The three cells below correspond to the forward, backward, and final passes from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE VIEW forward AS\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE VIEW backward AS\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_4c result_4c <<\n",
    "CREATE OR REPLACE VIEW likely_data AS\n",
    "...\n",
    ";\n",
    "SELECT * FROM likely_data WHERE run_size > 2 ORDER BY id, time LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_4c = %sqlcmd snippets query_4c\n",
    "grading_util.save_results(\"result_4c\", query_4c, result_4c)\n",
    "result_4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Granularity Transforms\n",
    "In this question we will write a roll-up query on an ontology. This requires a bit of background explanation.\n",
    "\n",
    "## [Tutorial, read] \n",
    "\n",
    "### The Brick Ontology\n",
    "\n",
    "An ontology is a way of showing the properties of a subject area and how they are related, by defining a set of terms and relational expressions that represent the entities in that subject area. \n",
    "\n",
    "The [Software-Defined Buildings](http://sdb.cs.berkeley.edu/sdb/) research project at Berkeley has led the development of a standard ontology for building metadata called [Brick](https://docs.brickschema.org/intro.html) that is getting a fair bit of attention in the world of IoT. Like many ontologies, it is represented as triples `(subject, predicate, object)`. In our database, the Brick ontology has been stored in a table called `ontology`.\n",
    "\n",
    "### The `SubClassOf` Predicate and the `Sensor` Class\n",
    "\n",
    "We are interested in readings from different classes of sensor devices. More specifically, we are interested in rows from the `metadata` table whose `metadata.class` entry maps to an `ontology` subject $s$, and that subject is in an ontology tuple `(`$s$`, http://www.w3.org/2000/01/rdf-schema#subClassOf, https://brickschema.org/schema/Brick#Sensor)`. Then we know that the sensor in `metadata` belongs to a sub-class of `Sensor`. A Subject is connected via Predicate to an Object. For example, a sensor child is connected via is `subClassOf` to its sensor parent. Unlike other trees that typically go from parent node down to leaf node, we are traversing from child node **up** to parent node.\n",
    "\n",
    "The diagram below shows a few of the `subject`s and `object`s from `ontology` in ovals. There is a dark arrow between two ovals if there is a corresponding row in `ontology`. The immediate sub-classes of `Sensor` in the diagram are shown in yellow; we'll call them \"Sensor children\".\n",
    "\n",
    "Thus, a sensor child is a sensor that is in the subclass of ***Sensor***. So, ***Sensor*** (Upper-cased) is it's parent. All of the children underneath ***Sensor*** and ***Sensor's*** children are also sensor children (like a family tree, where grandparent is ***Sensor***, parent is one of ***Sensor's*** children, and grandchild is also one of ***Sensor's*** children, and so is great-grandchild and so on). \n",
    "| Parent\n",
    "\n",
    "<img src=\"data/subClass.png\">\n",
    "\n",
    "This image is available as `data/subClass.png`, and can be made available full-screen for ease of access.\n",
    "\n",
    "### The `transitive_subClassOf` Relation\n",
    "\n",
    "The `transitive_subClassOf` materialized view is the set of edges that compose the ontology graph, i.e., all dark black and light green arrows on the diagram. It contains tuples of the form `(object, subject, hops, path)` where subject and object are connected transitively in `ontology` via one or more subClassOf predicates described above. `path` is a Postgres array type that shows the transitive path of class names through the ontology from subject to object, and `hops` is the length of that path. Run the next cell to see the first 4 rows of `transitive_subClassOf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM transitive_subClassOf LIMIT 4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transitive_subClassOf` edge graph is also called a [transitive closure](https://en.wikipedia.org/wiki/Transitive_closure) of the `subClassOf` predicate, because it represents that subclasses of subclasses of classes are sensors, and so on.\n",
    "\n",
    "We have already materialized this view for you, but to clarify its structure, we formed transitive “chains” by joining ontology with itself. For example, we could have made the materialized view `transitive_2edge_subClassOf` with the below command (Note that this materialized view does **not** exist.):\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW transitive_2edge_subClassOf AS\n",
    "    SELECT o1.subject, o2.object \n",
    "      FROM ontology o1 INNER JOIN ontology o2 ON o1.object = o2.subject\n",
    "     WHERE o1.predicate = 'http://www.w3.org/2000/01/rdf-schema#subClassOf'\n",
    "       AND o2.predicate = 'http://www.w3.org/2000/01/rdf-schema#subClassOf';\n",
    "``` \n",
    "\n",
    "Extending this example, in order to compute edges of length 3 we would have needed to join 3 references to `ontology`, and so on. To form all chains of arbitrary length requires the use of a recursive query—something we haven't learned in this class. Again, we have already done this for you in the materialized view `transitive_subClassOf` that provides the result of that recursive query. \n",
    "\n",
    "*Just for fun: If you're curious about the recursive query that computes this view, you can issue the command `\\d+ transitive_subClassOf` to Postgres. You may also want to read the documentation for SQL's [WITH RECURSIVE](https://www.postgresql.org/docs/9.1/queries-with.html) clause as implemented in Postgres.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5a\n",
    "\n",
    "We want to check the graph properties of the `subClassOf` predicate. It would be confusing if the `subClassOf` predicate had cycles!nt\n",
    "\n",
    "Write a query on `transitive_subClassOf` to check for cycles. Ask yourself this: what property in `transitive_subClassOf` would be indicative of a cycle? Your query should return one row of one boolean column: `true` if the predicate has cycles, `false` otherwise.\n",
    "\n",
    "**Hint:** You may find `EXISTS` to be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_5a result_5a <<\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_5a = %sqlcmd snippets query_5a\n",
    "grading_util.save_results(\"result_5a\", query_5a, result_5a)\n",
    "result_5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5b\n",
    "\n",
    "Assuming it's not cyclic, the next question is whether the `subClassOf` predicate forms *tree-shaped* connections only. The signature of a tree is that each node has at most one outbound edge (pointing to its \"parent\"). If any node has multiple outbound edges, the predicate forms a more general directed acyclic graph (a DAG). So we are looking to see if each subject is in a `subClassOf` predicate *with at most one object* (the single parent in the tree). \n",
    "\n",
    "Write a query that returns `true` if *each* subject is in a `subClassOf` predicate with at most one object, and `false` otherwise.\n",
    "\n",
    "**Hint:** Refer to how we created `transitive_2edge_subClassOf` in the tutorial to see how we check if something is in a `subClassOf` predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_5b result_5b <<\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_5b = %sqlcmd snippets query_5b\n",
    "grading_util.save_results(\"result_5b\", query_5b, result_5b)\n",
    "result_5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5c\n",
    "\n",
    "Now that we understand the graph properties of the ontology, let's use the ontology to do a roll-up as intended. Remember from [lecture 13](https://docs.google.com/presentation/d/19Nsksm89k8NBap9rwINJJFJeZCDSmF-feM6o0mz868c/edit?usp=sharing) that a roll-up is transforming to coarser grain (e.g., go up in a hierarchy). We're interested in the number of unique sensor `id`s from `metadata` that are transitively in subclasses of each \"Sensor child\" class. To compute this, you will have to associate each `metadata.id` with a matching `brickclass` *(if there is one!)*, and use the `transitive_subClassOf` view to identify all \"Sensor children\" for which the matching `brickclass` is transitively in a `subClass`. (If the `subClass` predicate is a DAG, then each `id` should be counted for *all* the direct subclasses of `Sensor` that it's transitively underneath.)\n",
    "\n",
    "Write a query that returns tuples of the form `(sensor_child, count)` that returns for each \"Sensor child\" the count of **distinct** `metadata.id` entries that are subclasses of that \"Sensor child\" class. Only output tuples that have a matching `brickclass`.\n",
    "\n",
    "**Hints:**\n",
    "- We are interested in \"the count of distinct metadata.id entries that are subclasses of that \"Sensor child\" class\". That means we are interested in all the sensors that are children of the sensors that have \"Sensor\" as its parent (these are the nodes in yellow in our diagram). It may be helpful to first create a CTE where you identify this set of unique sensor children according to these two properties (parent and predicate to specify that we are interested in the specific parent-child relationship and not other types of relationships) using information obtainable from the ontology table.\n",
    "\n",
    "Refer to `data/schema.png` for a refresher on the schema. This will be helpful to identify:\n",
    "- how you can associate sensors in `metadata` with a matching `brickclass` via `mapping`\n",
    "- how you can use the `transitive_subClassOf` view which has attributes `object` and `subject` just like `ontology` to match \"Sensor children\" with `brickclass`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_5c result_5c <<\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_5c = %sqlcmd snippets query_5c\n",
    "grading_util.save_results(\"result_5c\", query_5c, result_5c)\n",
    "result_5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have finished Project 3.\n",
    "\n",
    "Run the following cell to zip and download the results of your queries. You will also need to run the export cell at the end of the notebook.\n",
    "\n",
    "**Please save your notebook before exporting (this is a good time to do it!)** Otherwise, we may not be able to export your written responses to `proj3.pdf`. We will not be accepting regrade requests for failure to render written responses.\n",
    "\n",
    "**For your submission on Gradescope, you will only need to submit the single `proj3.zip` file generated by the export cell.** Please ensure that your submission `proj3.zip` file includes `proj3.pdf`, `proj3.ipynb`, and `results.zip`. \n",
    "\n",
    "**Please ensure that public tests pass upon submission.** It is your responsibility to wait until the autograder finishes running. We will not be accepting regrade requests for submission issues.\n",
    "\n",
    "**Common submission issues:** You MUST submit the generated zip file to the autograder. However, Safari is known to automatically unzip files upon downloading. You can fix this by going into Safari preferences, and deselect the box with the text \"Open safe files after downloading\" under the \"General\" tab. If you experience issues with downloading via clicking on the link, you can also navigate to the project 3 directory within JupyterHub (remove `proj3.ipynb` from the url), and manually download the generated zip files. Please post on Ed if you encounter any other submission issues.\n",
    "\n",
    "Run the following cell to zip and download the results of your queries. You will also need to run the export cell at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_util.prepare_submission_and_cleanup()  # builds results.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(files=['results.zip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_1b, result_1b_df = grading_util.load_results(\"result_1b\")\n>>> result_1b_df.shape == (5, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_1b, result_1b_df = grading_util.load_results(\"result_1b\")\n>>> len(str(result_1b_df.iloc[0, 1])) == 889\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_1c, result_1c_df = grading_util.load_results(\"result_1c\")\n>>> result_1c_df.shape == (4, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_1c, result_1c_df = grading_util.load_results(\"result_1c\")\n>>> len(str(result_1c_df.iloc[0, 1])) == 122\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_2a, result_2a_df = grading_util.load_results(\"result_2a\")\n>>> result_2a_df.shape == (100, 6)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2a, result_2a_df = grading_util.load_results(\"result_2a\")\n>>> result_2a_df.iloc[:50]['median'].sum() == 35684.7\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2a, result_2a_df = grading_util.load_results(\"result_2a\")\n>>> np.isclose(result_2a_df.iloc[:50]['mad'].sum(), 806.3000000000022)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df.shape == (100, 7)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> np.isclose(result_2b_df.iloc[:50, -1].sum(), 39270.961140000014)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_3b, result_3b_df = grading_util.load_results(\"result_3b\")\n>>> 0 <= result_3b_df.iloc[0, 0] <= 50\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_3b, result_3b_df = grading_util.load_results(\"result_3b\")\n>>> 0 <= result_3b_df.iloc[0, 1] <= 50\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_3c, result_3c_df = grading_util.load_results(\"result_3c\")\n>>> result_3c_df.iloc[0, 0] in [True, False]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_3c, result_3c_df = grading_util.load_results(\"result_3c\")\n>>> result_3c_df.shape == (1, 1)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e": {
     "name": "q3e",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_3e, result_3e_df = grading_util.load_results(\"result_3e\")\n>>> result_3e_df.sort_values(['clean_location', 'building_name', 'address']).shape == (5276, 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_3e, result_3e_df = grading_util.load_results(\"result_3e\")\n>>> result_3e_df.sort_values(['clean_location', 'building_name', 'address'])['clean_location'].iloc[-1] == 'SYSTEMWIDE'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_3e, result_3e_df = grading_util.load_results(\"result_3e\")\n>>> result_3e_df.sort_values(['clean_location', 'building_name', 'address'])['clean_location'].iloc[600] == 'BERKELEY'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_4a, result_4a_df = grading_util.load_results(\"result_4a\")\n>>> result_4a_df.shape == (10, 6)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_4a, result_4a_df = grading_util.load_results(\"result_4a\")\n>>> result_4a_df.iloc[:5]['lagvalue'].sum() == 593.052\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_4b, result_4b_df = grading_util.load_results(\"result_4b\")\n>>> result_4b_df.shape == (100, 3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_4b, result_4b_df = grading_util.load_results(\"result_4b\")\n>>> result_4b_df.iloc[:50]['value'].sum() == 3254484.79\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_4c, result_4c_df = grading_util.load_results(\"result_4c\")\n>>> result_4c_df.shape == (100, 10)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_4c, result_4c_df = grading_util.load_results(\"result_4c\")\n>>> np.isclose(result_4c_df.iloc[:50]['interpolated'].sum(), 195.94525) or np.isclose(result_4c_df.iloc[:50]['interpolated'].sum(), 369.7514880904479)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_5a, result_5a_df = grading_util.load_results(\"result_5a\")\n>>> result_5a_df.iloc[0, 0] in [True, False]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_5b, result_5b_df = grading_util.load_results(\"result_5b\")\n>>> result_5b_df.iloc[0, 0] in [True, False]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5c": {
     "name": "q5c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_5c, result_5c_df = grading_util.load_results(\"result_5c\")\n>>> result_5c_df.shape == (2, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_5c, result_5c_df = grading_util.load_results(\"result_5c\")\n>>> 0 <= result_5c_df.iloc[0, 1] <= 100\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_5c, result_5c_df = grading_util.load_results(\"result_5c\")\n>>> 3000 <= result_5c_df.iloc[1, 1] <= 4000\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
